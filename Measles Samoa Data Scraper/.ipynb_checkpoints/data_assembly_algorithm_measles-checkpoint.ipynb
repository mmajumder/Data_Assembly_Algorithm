{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measles Data Assembly Script\n",
    "*Maimuna S. Majumder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script utilizes the following libraries: \n",
    "# * requests, for making HTTP requests and pulling HTML content \n",
    "# * pandas, for organizing the data into an easily manipulatable table and then exporting said table \n",
    "# * regular expressions (re), for finding the relevant content on the source data page \n",
    "# * time, to suspend the script for a short period of time between each HTTP to avoid rate limiting\n",
    "# * sys, for error tracking\n",
    "import requests,pandas as pd,re,time,sys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the CSV as it exists currently\n",
    "df=pd.read_csv(\"DataOutput.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of source data URLs to have the script run on \n",
    "#For this work, these data are all sourced from archived tweets via the Samoan Government's Twitter account\n",
    "urls=[\n",
    "\"http://web.archive.org/web/20201210232948/https://twitter.com/samoagovt/status/1197790948178051074\",\n",
    "\"http://web.archive.org/web/20201210233007/https://twitter.com/samoagovt/status/1198133748312629248\",\n",
    "\"http://web.archive.org/web/20191127142955/https://twitter.com/samoagovt/status/1198810318136176641\",\n",
    "\"http://web.archive.org/web/20201210233135/https://twitter.com/samoagovt/status/1199106622083063808\",\n",
    "\"http://web.archive.org/web/20201210233155/https://twitter.com/samoagovt/status/1199490365130076160\",\n",
    "\"http://web.archive.org/web/20201210233211/https://twitter.com/samoagovt/status/1199844462479822848\",\n",
    "\"http://web.archive.org/web/20201210233315/https://twitter.com/samoagovt/status/1200179578745917442\",\n",
    "\"http://web.archive.org/web/20201210232349/https://twitter.com/samoagovt/status/1200591914031927296\",\n",
    "\"http://web.archive.org/web/20201125024235/https://twitter.com/samoagovt/status/1200923810439946240\",\n",
    "\"http://web.archive.org/web/20201125024433/https://twitter.com/samoagovt/status/1201285534368190464\",\n",
    "\"http://web.archive.org/web/20201210233617/https://twitter.com/samoagovt/status/1201637336515112960\",\n",
    "\"http://web.archive.org/web/20201210233709/https://twitter.com/samoagovt/status/1201979349374619648\",\n",
    "\"http://web.archive.org/web/20201125030102/https://twitter.com/samoagovt/status/1202335691310391296\",\n",
    "\"http://web.archive.org/web/20201210233943/https://twitter.com/samoagovt/status/1202707973048418304\",\n",
    "\"http://web.archive.org/web/20201210234054/https://twitter.com/samoagovt/status/1203077866646167552\",\n",
    "\"http://web.archive.org/web/20201210234205/https://twitter.com/samoagovt/status/1203604804997566464\",\n",
    "\"http://web.archive.org/web/20201210234328/https://twitter.com/samoagovt/status/1203793768182235136\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function (pageiterator) scans through the source data page content and looks for the regular expression that is passed in \n",
    "def pageiterator(regex,soup):\n",
    "    for x in soup.findAll(\"p\",{\"class\":\"TweetTextSize TweetTextSize--jumbo js-tweet-text tweet-text\"}):\n",
    "        no_bad_characters=x.text.replace(u'\\xa0', u' ') # The text replacement here cleans out bad characters \n",
    "        if(re.search(pattern=regex,string=no_bad_characters)):\n",
    "            return(re.findall(regex,no_bad_characters))\n",
    "    for x in soup.findAll(\"div\"):\n",
    "        no_bad_characters=x.text.replace(u'\\xa0', u' ')\n",
    "        if(re.search(pattern=regex,string=no_bad_characters)):\n",
    "            return(re.findall(regex,no_bad_characters))\n",
    "    return[[\"not found\",\"not found 2\",\"not found 3\"]] # If nothing is found, the function returns a string saying \"not found\", numbered based off of which output of the regular expression the call to the function is looking for \n",
    "\n",
    "#For this work, pull data for the field \"cumulative cases\"\n",
    "def get_field_two(mytext):\n",
    "    regex = r\"([+-]?[0-9]*[,]?[0-9]+)( measles cases)\"\n",
    "    return(next(string for string in pageiterator(regex,mytext)[0]))\n",
    "\n",
    "#For this work, pull data for the field \"incident cases\", which includes only cases that have been recorded in last 24 hours\n",
    "def get_field_three(mytext):\n",
    "    regex = r\"([+-]?[0-9]*[,]?[0-9]+)( recorded in the last 24 hours)\"\n",
    "    return(next(string for string in pageiterator(regex,mytext)[0]))\n",
    "\n",
    "#For this work, pull data for the field \"cumulative deaths\"\n",
    "def get_field_four(mytext):\n",
    "    regex = r\"([+-]?[0-9]*[,]?[0-9]+)( measles related deaths)\"\n",
    "    return(next(string for string in pageiterator(regex,mytext)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://web.archive.org/web/20201210232948/https://twitter.com/samoagovt/status/1197790948178051074\n",
      "http://web.archive.org/web/20201210233007/https://twitter.com/samoagovt/status/1198133748312629248\n",
      "http://web.archive.org/web/20191127142955/https://twitter.com/samoagovt/status/1198810318136176641\n",
      "http://web.archive.org/web/20201210233135/https://twitter.com/samoagovt/status/1199106622083063808\n",
      "http://web.archive.org/web/20201210233155/https://twitter.com/samoagovt/status/1199490365130076160\n",
      "http://web.archive.org/web/20201210233211/https://twitter.com/samoagovt/status/1199844462479822848\n",
      "http://web.archive.org/web/20201210233315/https://twitter.com/samoagovt/status/1200179578745917442\n",
      "http://web.archive.org/web/20201210232349/https://twitter.com/samoagovt/status/1200591914031927296\n",
      "http://web.archive.org/web/20201125024235/https://twitter.com/samoagovt/status/1200923810439946240\n",
      "http://web.archive.org/web/20201125024433/https://twitter.com/samoagovt/status/1201285534368190464\n",
      "http://web.archive.org/web/20201210233617/https://twitter.com/samoagovt/status/1201637336515112960\n",
      "http://web.archive.org/web/20201210233709/https://twitter.com/samoagovt/status/1201979349374619648\n",
      "http://web.archive.org/web/20201125030102/https://twitter.com/samoagovt/status/1202335691310391296\n",
      "http://web.archive.org/web/20201210233943/https://twitter.com/samoagovt/status/1202707973048418304\n",
      "http://web.archive.org/web/20201210234054/https://twitter.com/samoagovt/status/1203077866646167552\n",
      "http://web.archive.org/web/20201210234205/https://twitter.com/samoagovt/status/1203604804997566464\n",
      "http://web.archive.org/web/20201210234328/https://twitter.com/samoagovt/status/1203793768182235136\n"
     ]
    }
   ],
   "source": [
    "# This is where the main execution of the script happens; a for loop runs through the list of URLs, makes requests to the pages, populates the table based on results, and reports each URL as the script iterates  \n",
    "for url in urls:\n",
    "    try:\n",
    "        req_object=requests.get(url)\n",
    "        thetext=req_object.text\n",
    "        soup=BeautifulSoup(thetext)\n",
    "        if(df.empty): #If the table is completely empty, this script starts populating the table with the first date of reporting (2019-11-22)\n",
    "            df=df.append({'date':pd.Timestamp(2019,11,22),'cumulative_cases':get_field_two(soup),'incident_cases':get_field_three(soup),'cumulative_deaths':get_field_four(soup),'source_url':url},ignore_index=True)\n",
    "        else:\n",
    "            df=df.append({'date':df.tail(1)['date'].values[0]+pd.to_timedelta(1, unit='D'),'cumulative_cases':get_field_two(soup),'incident_cases':get_field_three(soup),'cumulative_deaths':get_field_four(soup),'source_url':url},ignore_index=True)\n",
    "        print(url)\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cumulative_cases</th>\n",
       "      <th>incident_cases</th>\n",
       "      <th>cumulative_deaths</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>1,644</td>\n",
       "      <td>202</td>\n",
       "      <td>20</td>\n",
       "      <td>http://web.archive.org/web/20201210232948/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-23</td>\n",
       "      <td>1,797</td>\n",
       "      <td>153</td>\n",
       "      <td>22</td>\n",
       "      <td>http://web.archive.org/web/20201210233007/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2,194</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>http://web.archive.org/web/20191127142955/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-25</td>\n",
       "      <td>2,437</td>\n",
       "      <td>243</td>\n",
       "      <td>32</td>\n",
       "      <td>http://web.archive.org/web/20201210233135/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-26</td>\n",
       "      <td>2,686</td>\n",
       "      <td>249</td>\n",
       "      <td>33</td>\n",
       "      <td>http://web.archive.org/web/20201210233155/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-11-27</td>\n",
       "      <td>2,936</td>\n",
       "      <td>250</td>\n",
       "      <td>39</td>\n",
       "      <td>http://web.archive.org/web/20201210233211/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>3,149</td>\n",
       "      <td>213</td>\n",
       "      <td>42</td>\n",
       "      <td>http://web.archive.org/web/20201210233315/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>http://web.archive.org/web/20201210232349/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>http://web.archive.org/web/20201125024235/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>http://web.archive.org/web/20201125024433/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>3,881</td>\n",
       "      <td>153</td>\n",
       "      <td>55</td>\n",
       "      <td>http://web.archive.org/web/20201210233617/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>4,052</td>\n",
       "      <td>171</td>\n",
       "      <td>60</td>\n",
       "      <td>http://web.archive.org/web/20201210233709/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>not found</td>\n",
       "      <td>http://web.archive.org/web/20201125030102/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-12-05</td>\n",
       "      <td>4,357</td>\n",
       "      <td>140</td>\n",
       "      <td>63</td>\n",
       "      <td>http://web.archive.org/web/20201210233943/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>4,460</td>\n",
       "      <td>103</td>\n",
       "      <td>65</td>\n",
       "      <td>http://web.archive.org/web/20201210234054/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-12-07</td>\n",
       "      <td>4,581</td>\n",
       "      <td>121</td>\n",
       "      <td>68</td>\n",
       "      <td>http://web.archive.org/web/20201210234205/http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-12-08</td>\n",
       "      <td>4,693</td>\n",
       "      <td>112</td>\n",
       "      <td>70</td>\n",
       "      <td>http://web.archive.org/web/20201210234328/http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date cumulative_cases incident_cases cumulative_deaths  \\\n",
       "0  2019-11-22            1,644            202                20   \n",
       "1  2019-11-23            1,797            153                22   \n",
       "2  2019-11-24            2,194            144                25   \n",
       "3  2019-11-25            2,437            243                32   \n",
       "4  2019-11-26            2,686            249                33   \n",
       "5  2019-11-27            2,936            250                39   \n",
       "6  2019-11-28            3,149            213                42   \n",
       "7  2019-11-29        not found      not found         not found   \n",
       "8  2019-11-30        not found      not found         not found   \n",
       "9  2019-12-01        not found      not found         not found   \n",
       "10 2019-12-02            3,881            153                55   \n",
       "11 2019-12-03            4,052            171                60   \n",
       "12 2019-12-04        not found      not found         not found   \n",
       "13 2019-12-05            4,357            140                63   \n",
       "14 2019-12-06            4,460            103                65   \n",
       "15 2019-12-07            4,581            121                68   \n",
       "16 2019-12-08            4,693            112                70   \n",
       "\n",
       "                                           source_url  \n",
       "0   http://web.archive.org/web/20201210232948/http...  \n",
       "1   http://web.archive.org/web/20201210233007/http...  \n",
       "2   http://web.archive.org/web/20191127142955/http...  \n",
       "3   http://web.archive.org/web/20201210233135/http...  \n",
       "4   http://web.archive.org/web/20201210233155/http...  \n",
       "5   http://web.archive.org/web/20201210233211/http...  \n",
       "6   http://web.archive.org/web/20201210233315/http...  \n",
       "7   http://web.archive.org/web/20201210232349/http...  \n",
       "8   http://web.archive.org/web/20201125024235/http...  \n",
       "9   http://web.archive.org/web/20201125024433/http...  \n",
       "10  http://web.archive.org/web/20201210233617/http...  \n",
       "11  http://web.archive.org/web/20201210233709/http...  \n",
       "12  http://web.archive.org/web/20201125030102/http...  \n",
       "13  http://web.archive.org/web/20201210233943/http...  \n",
       "14  http://web.archive.org/web/20201210234054/http...  \n",
       "15  http://web.archive.org/web/20201210234205/http...  \n",
       "16  http://web.archive.org/web/20201210234328/http...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This simple block is just to render the content of the table after the script runs \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send dataframe to a CSV and Excel file\n",
    "df.to_csv(\"DataOutput.csv\",index=False)\n",
    "df.to_excel(\"DataExcel.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only execute the below to reset the CSV\n",
    "Uncomment the code before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.DataFrame(columns=[\"date\",\"cumulative_cases\",\"incident_cases\",\"cumulative_deaths\", \"source_url\"])\n",
    "#df.to_csv(\"DataOutput.csv\",index=False)\n",
    "#df.to_excel(\"DataExcel.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
